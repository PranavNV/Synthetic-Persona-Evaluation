{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de207b0-c5f5-40fe-bf08-43d44f2442a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c3ee2-1b64-482e-9678-b8ed6d0b859b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab995b4c-5060-4f87-a9cf-a9a3cf244228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_Post_Processing/llm_responses_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0521d-980e-4801-b163-86bede3141ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891cc4e-763e-44e8-9a27-7860c935afb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a20e7-b393-426d-b2bd-bf651b6b0bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d45e2b1-9418-47e0-b519-f4833b15589e",
   "metadata": {},
   "source": [
    "# Marked Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72a4ce-4cfd-46c1-af60-69cd6dd4300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_odds(df1, df2, df0, verbose=False, lower=True):\n",
    "    \"\"\"Calculate log-odds ratio for words in df1 (target group) vs df2 (non-target group).\"\"\"\n",
    "    # Tokenize and count words\n",
    "    if lower:\n",
    "        counts1 = defaultdict(int, [[i, j] for i, j in df1.str.lower().str.split(expand=True).stack().replace('[^a-zA-Z\\s]', '', regex=True).value_counts().items()])\n",
    "        counts2 = defaultdict(int, [[i, j] for i, j in df2.str.lower().str.split(expand=True).stack().replace('[^a-zA-Z\\s]', '', regex=True).value_counts().items()])\n",
    "        prior = defaultdict(int, [[i, j] for i, j in df0.str.lower().str.split(expand=True).stack().replace('[^a-zA-Z\\s]', '', regex=True).value_counts().items()])\n",
    "    \n",
    "    else:\n",
    "        counts1 = defaultdict(int, [[i, j] for i, j in df1.str.split(expand=True).stack().replace('[^a-zA-Z\\s]', '', regex=True).value_counts().items()])\n",
    "        counts2 = defaultdict(int, [[i, j] for i, j in df2.str.split(expand=True).stack().replace('[^a-zA-Z\\s]', '', regex=True).value_counts().items()])\n",
    "        prior = defaultdict(int, [[i, j] for i, j in df0.str.split(expand=True).stack().replace('[^a-zA-Z\\s]', '', regex=True).value_counts().items()])\n",
    "\n",
    "    # Calculate log-odds ratio\n",
    "    sigmasquared = defaultdict(float)\n",
    "    sigma = defaultdict(float)\n",
    "    delta = defaultdict(float)\n",
    "\n",
    "    for word in prior.keys():\n",
    "        prior[word] = int(prior[word] + 0.5)\n",
    "\n",
    "    for word in counts2.keys():\n",
    "        counts1[word] = int(counts1[word] + 0.5)\n",
    "        if prior[word] == 0:\n",
    "            prior[word] = 1\n",
    "\n",
    "    for word in counts1.keys():\n",
    "        counts2[word] = int(counts2[word] + 0.5)\n",
    "        if prior[word] == 0:\n",
    "            prior[word] = 1\n",
    "\n",
    "    n1 = sum(counts1.values())\n",
    "    n2 = sum(counts2.values())\n",
    "    nprior = sum(prior.values())\n",
    "\n",
    "    for word in prior.keys():\n",
    "        if prior[word] > 0:\n",
    "            l1 = float(counts1[word] + prior[word]) / ((n1 + nprior) - (counts1[word] + prior[word]))\n",
    "            l2 = float(counts2[word] + prior[word]) / ((n2 + nprior) - (counts2[word] + prior[word]))\n",
    "            sigmasquared[word] = 1 / (float(counts1[word]) + float(prior[word])) + 1 / (float(counts2[word]) + float(prior[word]))\n",
    "            sigma[word] = math.sqrt(sigmasquared[word])\n",
    "            delta[word] = (math.log(l1) - math.log(l2)) / sigma[word]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Top 10 words with highest log-odds (distinctive for target group):\")\n",
    "        for word in sorted(delta, key=delta.get, reverse=True)[:20]:\n",
    "            print(f\"{word}: {delta[word]:.3f}\")\n",
    "\n",
    "        print(\"\\nTop 10 words with lowest log-odds (distinctive for non-target group):\")\n",
    "        for word in sorted(delta, key=delta.get)[:20]:\n",
    "            print(f\"{word}: {delta[word]:.3f}\")\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be82a54c-1cea-4264-b697-a8ad4e4672dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marked_words(df, target_race, non_target_race=\"White\", verbose=False):\n",
    "    \"\"\"Identify words that distinguish the target race group from the non-target race group.\"\"\"\n",
    "    # Filter data for target and non-target groups\n",
    "    target_df = df[df['Race'] == target_race]\n",
    "    non_target_df = df[df['Race'] == non_target_race]\n",
    "\n",
    "    # Calculate log-odds\n",
    "    delta = get_log_odds(target_df['Response'], non_target_df['Response'], df['Response'], verbose=verbose)\n",
    "\n",
    "    # Filter significant words (z-score > 1.96 or < -1.96)\n",
    "    significant_words = {word: score for word, score in delta.items() if abs(score) > 1.96}\n",
    "\n",
    "    return significant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6131c11-e827-4173-8a0e-a817e19a1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target races\n",
    "target_races = [\n",
    "    'African American or Black',\n",
    "    'Asian',\n",
    "    'Native Hawaiian or Other Pacific Islander',\n",
    "    'American Indian or Alaska Native',\n",
    "    'Hispanic or Latino'\n",
    "]\n",
    "\n",
    "# Calculate log-odds for each target group\n",
    "results = {}\n",
    "for race in target_races:\n",
    "    print(f\"Calculating log-odds for {race}...\")\n",
    "    results[race] = marked_words(df, target_race=race, verbose=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75b62f-a6cd-4825-be54-43f2192445c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c09a8-a90a-40e9-b0ab-8301d67f1843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda96cb6-3343-4fed-ba5b-9cabc8419cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0952dc-ede8-4610-aad1-4431f94f537c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca939148-1ce6-4fc6-908e-6f95af993da8",
   "metadata": {},
   "source": [
    "# Semantic Surprisal Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedb83b-88cb-4770-94ca-cf5563debba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained model for semantic similarity\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "df = pd.read_csv('Data_Post_Processing/Participants_Response_Clean.csv')\n",
    "\n",
    "def calculate_surprisal(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_fragments = len(sentences)\n",
    "    \n",
    "    if num_fragments < 2:\n",
    "        return 0  # No surprisal if there's only one sentence\n",
    "    \n",
    "    # Compute sentence embeddings\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate semantic distances between consecutive sentences\n",
    "    distances = []\n",
    "    for i in range(1, num_fragments):\n",
    "        distance = 1 - util.pytorch_cos_sim(embeddings[i], embeddings[i-1]).item()\n",
    "        distances.append(distance)\n",
    "    \n",
    "    # Calculate the average semantic distance\n",
    "    avg_distance = np.mean(distances)\n",
    "    \n",
    "    # Normalize the surprisal in the [0, 2] space\n",
    "    surprisal = (2 / (num_fragments - 1)) * np.sum(distances)\n",
    "    \n",
    "    return surprisal\n",
    "\n",
    "def calculate_surprisal_for_dataframe(df):\n",
    "    # Apply the surprisal calculation to each row in the dataframe\n",
    "    df['Surprisal'] = df['Response'].apply(calculate_surprisal)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Calculate surprisal for each response\n",
    "df = calculate_surprisal_for_dataframe(df)\n",
    "\n",
    "# Group by race and calculate average surprisal for each race group\n",
    "race_surprisal = df.groupby('Race')['Surprisal'].mean().reset_index()\n",
    "\n",
    "# Print the results\n",
    "print(\"Average Surprisal by Race:\")\n",
    "print(race_surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e363a-1efa-44bb-b3ce-d5e8a2d4c26c",
   "metadata": {},
   "source": [
    "# Semantic Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c45e9-07fb-4741-ae23-5f3bc1498c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the sentence embedding model\n",
    "model = SentenceTransformer('thenlper/gte-large')\n",
    "\n",
    "def semantic_distance(texts):\n",
    "    \"\"\"\n",
    "    Compute the pairwise semantic distance matrix for a list of texts.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return 1 - similarity_matrix\n",
    "\n",
    "def inverse_homogenization_score(responses, sample_size=None):\n",
    "    \"\"\"\n",
    "    Calculate the inverse homogenization score for a set of responses.\n",
    "    Optionally, sample a subset of responses to reduce computation.\n",
    "    \"\"\"\n",
    "    if sample_size and len(responses) > sample_size:\n",
    "        responses = np.random.choice(responses, size=sample_size, replace=False)\n",
    "    \n",
    "    n = len(responses)\n",
    "    if n < 2:\n",
    "        return 0  # Not enough responses to compute diversity\n",
    "    \n",
    "    distance_matrix = semantic_distance(responses)\n",
    "    upper_triangle = np.triu(distance_matrix, k=1)\n",
    "    average_distance = np.sum(upper_triangle) / (n * (n - 1) / 2)\n",
    "    \n",
    "    return average_distance\n",
    "\n",
    "def calculate_semantic_diversity_by_race(df, race_column, response_column, sample_size=None):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity of responses for each race group in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - race_column: Name of the column containing race information.\n",
    "    - response_column: Name of the column containing responses.\n",
    "    - sample_size: Optional. Number of responses to sample for computation.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with race groups as keys and their semantic diversity scores as values.\n",
    "    \"\"\"\n",
    "    # Get unique race groups\n",
    "    race_groups = df[race_column].unique()\n",
    "    \n",
    "    # Dictionary to store diversity scores\n",
    "    diversity_scores = {}\n",
    "    \n",
    "    # Calculate semantic diversity for each race group\n",
    "    for race in race_groups:\n",
    "        # Filter responses for the current race group\n",
    "        filtered_responses = df[df[race_column] == race][response_column].tolist()\n",
    "        \n",
    "        # Calculate semantic diversity\n",
    "        diversity_score = inverse_homogenization_score(filtered_responses, sample_size)\n",
    "        \n",
    "        # Store the result\n",
    "        diversity_scores[race] = diversity_score\n",
    "    \n",
    "    return diversity_scores\n",
    "\n",
    "\n",
    "#Load the data\n",
    "\n",
    "df = pd.read_csv('Data_Post_Processing/Participants_Response_Clean.csv')\n",
    "\n",
    "\n",
    "# Calculate semantic diversity for each race group\n",
    "diversity_scores = calculate_semantic_diversity_by_race(df, race_column='Race', response_column='Response')\n",
    "\n",
    "# Print the results\n",
    "for race, score in diversity_scores.items():\n",
    "    print(f\"Semantic diversity for {race}: {score*10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493afbc-50e5-4447-a521-cdaad63060fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_diversity_whole_corpus(df, response_column, sample_size=None):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity of the whole corpus (all responses).\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - response_column: Name of the column containing responses.\n",
    "    - sample_size: Optional. Number of responses to sample for computation.\n",
    "    \n",
    "    Returns:\n",
    "    - Semantic diversity score for the whole corpus.\n",
    "    \"\"\"\n",
    "    # Get all responses\n",
    "    all_responses = df[response_column].tolist()\n",
    "    \n",
    "    # Calculate semantic diversity\n",
    "    diversity_score = inverse_homogenization_score(all_responses, sample_size)\n",
    "    \n",
    "    return diversity_score\n",
    "    \n",
    "calculate_semantic_diversity_whole_corpus(df, response_column='Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f45273af-659d-4ce9-8c35-eec53d382f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data_Post_Processing/Participants_Response_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb404d3d-8760-485e-9d2b-fe39443c2ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32828d-f4b1-42af-9434-d373224c5c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f647744-b218-4340-a7f8-c77a240d6016",
   "metadata": {},
   "source": [
    "# Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63252c5c-2148-4af6-be53-dc9f530840dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_Post_Processing/Participants_Response_Clean.csv')\n",
    "\n",
    "#Semantic Novelty\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode all responses\n",
    "df[\"Embedding\"] = df[\"Response\"].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Function to compute average pairwise distance\n",
    "def avg_semantic_distance(embeddings):\n",
    "    if len(embeddings) < 2:\n",
    "        return 0.0  # Not enough stories to compare\n",
    "    distances = []\n",
    "    for emb1, emb2 in combinations(embeddings, 2):\n",
    "        similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        distances.append(1 - similarity)  # Convert similarity to distance\n",
    "    return np.mean(distances)\n",
    "\n",
    "# Calculate average distance for each race group\n",
    "race_groups = df.groupby(\"Race\")[\"Embedding\"].apply(list)\n",
    "race_distances = {race: avg_semantic_distance(embeddings) for race, embeddings in race_groups.items()}\n",
    "\n",
    "# Calculate average distance for the entire corpus\n",
    "corpus_distance = avg_semantic_distance(df[\"Embedding\"].tolist())\n",
    "\n",
    "# Compute novelty for each race group\n",
    "novelty_scores = {race: 2 * abs(distance - corpus_distance) for race, distance in race_distances.items()}\n",
    "\n",
    "print(\"Semantic Novelty by Race Group:\")\n",
    "for race, novelty in novelty_scores.items():\n",
    "    print(f\"Race {race}: {novelty:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd4ab65-d90d-4425-aa67-45056540b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data_Post_Processing/Participants_Response_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c43c69-4187-488a-b8cf-6d03398229b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e928d79-3804-4a01-9596-62da85c9b0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebf2bc43-f02c-438b-b374-66ea7a233daf",
   "metadata": {},
   "source": [
    "# Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971168c4-2c0a-4411-86ab-1d30fa7d8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv('Data_Post_Processing/Participants_Response_Clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b8d32-4410-4cea-8019-4f60669ae4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDG Semantic complexity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Compute TF-IDF for the entire corpus\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df[\"Response\"])\n",
    "\n",
    "# Average TF-IDF score per response (higher = more rare terms)\n",
    "df[\"TFIDF_Complexity\"] = np.array(X.mean(axis=1)).flatten()\n",
    "\n",
    "# Group by race and average\n",
    "tfidf_complexity = df.groupby(\"Race\")[\"TFIDF_Complexity\"].mean()\n",
    "print(\"Semantic Complexity (TF-IDF):\\n\", tfidf_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad953af1-d26b-48b7-8ff0-b01e6ef791ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec Semantic Complexity\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess(text):\n",
    "    return [word.lower() for word in word_tokenize(text) if word.isalpha()]\n",
    "\n",
    "# Train Word2Vec on the corpus\n",
    "sentences = [preprocess(text) for text in df[\"Response\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to compute semantic spread\n",
    "def semantic_spread(text):\n",
    "    words = preprocess(text)\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) < 2:\n",
    "        return 0.0\n",
    "    distances = [cosine(vectors[i], vectors[j]) for i in range(len(vectors)) for j in range(i+1, len(vectors))]\n",
    "    return np.mean(distances) if distances else 0.0\n",
    "\n",
    "# Compute for each response\n",
    "df[\"Word2Vec_Complexity\"] = df[\"Response\"].apply(semantic_spread)\n",
    "\n",
    "# Group by race\n",
    "w2v_complexity = df.groupby(\"Race\")[\"Word2Vec_Complexity\"].mean()\n",
    "print(\"Semantic Complexity (Word2Vec Spread):\\n\", w2v_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca6653c-66b4-42b5-90c3-d0ce78909cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b0790-a07d-40e1-a2c2-e7003657e433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d4bc4-6816-47a6-9f30-b8ba1cd3cceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5497f-2b50-4a77-bc8e-b98e97c87323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Combined_Complexity\"] = 0.5 * (df[\"TFIDF_Complexity\"] / df[\"TFIDF_Complexity\"].max()) + \\\n",
    "                           0.5 * (df[\"Word2Vec_Complexity\"] / df[\"Word2Vec_Complexity\"].max())\n",
    "\n",
    "group_complexity = df.groupby(\"Race\")[\"Combined_Complexity\"].mean()\n",
    "print(\"Combined Semantic Complexity:\\n\", group_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13529005-f795-41c6-a792-4ef56fb49c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data_Post_Processing/Participants_Response_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218971c7-c14d-4c3a-a60f-cfb297f8f087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
