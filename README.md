# Synthetic Persona Evaluation

As LLMs (large language models) are increasingly used to generate synthetic personas—particularly in data-limited domains such as health, privacy, and HCI—it becomes necessary to understand how these narratives represent identity, especially that of minority communities. 
In this project, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek v2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed-methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1,512 LLM-generated persona to human-authored responses. 
Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. 
We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic.

The code in this project denotes the method used to evaluate and audit the synthetic persona to the human written responses obtained. 
For this we use two frameworks:
1. Framework of Markedness: Using log-odd ratio, we evaluate how minority representations devaiate from the benchmark majority
2. Framework of Creativity: Using four metrics of creativity (Novelty, Complexity, Diversity and Entropy/Surprisal, we evaluate the content, theme and writing style of synthetic personas.

The code in this project depicts the quantification of the above frameworks to audit these personas. 
